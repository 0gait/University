{"cells":[{"cell_type":"markdown","source":["# Example: Spark over Cosmos DB\n\nA notebook is composed by a sequence of cells. Cells can have text in markdown or code - code cells will be in the default programming language.\nText cells in markdown must start with ```%md```.\n\nThis is a first example showing how to execute Spark programs that access Cosmos DB database. The program will compute the number of auctions for each user (**note**: this kind of computation would be better performed using CosmosDB query interface -- we are using it only for demonstration).\n\nWe will have serveral versions:\n* first version uses the dataframes interface and displays the result in the notebook;\n* second version uses the dataframes SQL interface and displays the result in the notebook;\n* last version adds writing the results back into CosmosDB.\n\nFor using the connection to CosmosDB, you need to install the CosmosDB Spark library in the cluster. The library is available here:\n[https://search.maven.org/remotecontent?filepath=com/azure/cosmos/spark/azure-cosmos-spark_3-2_2-12/4.14.1/azure-cosmos-spark_3-2_2-12-4.14.1.jar](https://search.maven.org/remotecontent?filepath=com/azure/cosmos/spark/azure-cosmos-spark_3-2_2-12/4.14.1/azure-cosmos-spark_3-2_2-12-4.14.1.jar)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be510a19-abe6-4d9d-9c95-ede60cb79bec"}}},{"cell_type":"markdown","source":["## Program using DataFrames interface\n\nEnvironment has a number of variable defined by default:\n* **spark** : SparkSession [https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.SparkSession.html#pyspark.sql.SparkSession](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.SparkSession.html#pyspark.sql.SparkSession)\n* **sc** : SparkContext [https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.SparkContext.html](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.SparkContext.html)\n\nThe following program reads data from auctions container and computes how many auctions are owned by each user."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"84a0aab6-ea5d-4f85-87a8-88b5f3d01434"}}},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\ntry:\n  # Configuration for accessing CosmosDB\n  readConfig = {\n    \"spark.cosmos.accountEndpoint\": \"https://scc23nmp.documents.azure.com:443/\",\n    \"spark.cosmos.accountKey\": \"3Stlau3NbZIGs97x5KOW8J9byciE0GxuiN3B3P8ykzivdccUzRhrp6xMUQ4SOHmQBGZWcJqK7WsPDalVZwXQcQ==\",\n    \"spark.cosmos.database\": \"scc23dbnmp\",\n    \"spark.cosmos.container\": \"auctions\",\n      # Getting only the id and owner from the auctions\n    \"spark.cosmos.read.customQuery\": \"SELECT a.id, a.owner FROM auctions a\"\n  }\n\n  # Connect via azure-cosmosdb-spark to create Spark DataFrame\n  # Infer schema automaticaly\n  auctions = spark.read.format(\"cosmos.oltp\").options(**readConfig) \\\n                    .option(\"spark.cosmos.read.inferSchema.enabled\", \"true\") \\\n                    .load()\n\n  display(auctions)\n    # Count the number of auctions for each owner\n  result = auctions.groupBy(\"owner\") \\\n              .count()\n  display(result)\nexcept Exception as e:\n  print(e)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"91b8ae24-3688-45b8-9456-f2940ffd6237"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["21332","nmp"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":[],"pivotAggregation":null,"xColumns":[],"yColumns":[]},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"id","type":"\"string\"","metadata":"{}"},{"name":"owner","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>owner</th></tr></thead><tbody><tr><td>21332</td><td>nmp</td></tr></tbody></table></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["nmp",1]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":[],"pivotAggregation":null,"xColumns":[],"yColumns":[]},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"owner","type":"\"string\"","metadata":"{}"},{"name":"count","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>owner</th><th>count</th></tr></thead><tbody><tr><td>nmp</td><td>1</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Program using SQL interface\n\nThis program uses SQL interface for executing the same operations as before."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"06a7b85e-29d2-43aa-a203-762a36508656"}}},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\ntry:\n  # Configuration for accessing CosmosDB\n  readConfig = {\n    \"spark.cosmos.accountEndpoint\": \"https://scc23nmp.documents.azure.com:443/\",\n    \"spark.cosmos.accountKey\": \"3Stlau3NbZIGs97x5KOW8J9byciE0GxuiN3B3P8ykzivdccUzRhrp6xMUQ4SOHmQBGZWcJqK7WsPDalVZwXQcQ==\",\n    \"spark.cosmos.database\": \"scc23dbnmp\",\n    \"spark.cosmos.container\": \"auctions\",\n      # Getting only the id and owner from the auctions\n    \"spark.cosmos.read.customQuery\": \"SELECT a.id, a.owner FROM auctions a\"\n  }\n\n  # Connect via azure-cosmosdb-spark to create Spark DataFrame\n  # Infer schema automaticaly\n  auctions = spark.read.format(\"cosmos.oltp\").options(**readConfig) \\\n                    .option(\"spark.cosmos.read.inferSchema.enabled\", \"true\") \\\n                    .load()\n  # Let's register the dataframe as a view\n  auctions.createOrReplaceTempView(\"auctions\")\n\n    # Count the number of auctions for each owner\n  result = spark.sql(\"\"\"SELECT owner, count(*) AS count FROM auctions\n                          GROUP BY owner\"\"\")\n  display(result)\nexcept Exception as e:\n  print(e)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb7b5200-4dea-4e26-8185-bc967b706f7a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["nmp",1]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":[],"pivotAggregation":null,"xColumns":[],"yColumns":[]},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"owner","type":"\"string\"","metadata":"{}"},{"name":"count","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>owner</th><th>count</th></tr></thead><tbody><tr><td>nmp</td><td>1</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Execute computation and dump result\n\nWrite result in collection **auctionsFreq**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b10d56ec-c761-4dec-a936-b5808763e9db"}}},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\ntry:\n  # Configuration for accessing CosmosDB\n  readConfig = {\n    \"spark.cosmos.accountEndpoint\": \"https://scc23nmp.documents.azure.com:443/\",\n    \"spark.cosmos.accountKey\": \"3Stlau3NbZIGs97x5KOW8J9byciE0GxuiN3B3P8ykzivdccUzRhrp6xMUQ4SOHmQBGZWcJqK7WsPDalVZwXQcQ==\",\n    \"spark.cosmos.database\": \"scc23dbnmp\",\n    \"spark.cosmos.container\": \"auctions\",\n      # Getting only the id and owner from the auctions\n    \"spark.cosmos.read.customQuery\": \"SELECT a.id, a.owner FROM auctions a\"\n  }\n\n  # Connect via azure-cosmosdb-spark to create Spark DataFrame\n  # Infer schema automaticaly\n  auctions = spark.read.format(\"cosmos.oltp\").options(**readConfig) \\\n                    .option(\"spark.cosmos.read.inferSchema.enabled\", \"true\") \\\n                    .load()\n  # Let's register the dataframe as a view\n  auctions.createOrReplaceTempView(\"auctions\")\n\n    # Count the number of auctions for each owner\n  result = spark.sql(\"\"\"SELECT owner, count(*) AS count FROM auctions\n                          GROUP BY owner\"\"\")\n\n  # Write configuration\n  writeConfig = {\n    \"spark.cosmos.accountEndpoint\": \"https://scc23nmp.documents.azure.com:443/\",\n    \"spark.cosmos.accountKey\": \"3Stlau3NbZIGs97x5KOW8J9byciE0GxuiN3B3P8ykzivdccUzRhrp6xMUQ4SOHmQBGZWcJqK7WsPDalVZwXQcQ==\",\n    \"spark.cosmos.database\": \"scc23dbnmp\",\n    \"spark.cosmos.container\": \"auctionsFreq\",\n  }\n\n  # Write to Cosmos DB from the result DataFrame\n  result.write.format(\"cosmos.oltp\").options(**writeConfig) \\\n              .mode(\"append\") \\\n              .save()\nexcept Exception as e:\n  print(e)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c6decd0b-2bf2-4ca7-b9ef-030b018d7fb2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Cosmos DB first example","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1994063266875019}},"nbformat":4,"nbformat_minor":0}
